{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    },
    "colab": {
      "name": "classification_example.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ymuto0302/semi_2020/blob/main/classification_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyjOZbeLGehs"
      },
      "source": [
        "## (準備) Google Drive のマウント"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8vh42uL-xZP",
        "outputId": "e6092b2e-3171-4fae-df4c-ba010176eecc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1N5f6Qo-xZU",
        "outputId": "799546e6-1343-44c2-f272-c217802fac0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls /content/drive/\"My Drive\"/Semi2020/LivedoorNewsCorpus/text"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dokujo-tsushin\tkaden-channel\tmovie-enter  smax\t   topic-news\n",
            "it-life-hack\tlivedoor-homme\tpeachy\t     sports-watch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4dp-qGZAlsB"
      },
      "source": [
        "## 以下のサイトを参考に（コピペして）MeCab + Neologd をインストールする\n",
        "Google ColabにMeCabとipadic-NEologdをインストールする\n",
        "\n",
        "https://qiita.com/jun40vn/items/78e33e29dce3d50c2df1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BMDBGHx-xZX",
        "outputId": "83dcd8d6-4317-4dfa-ee6d-f76554732218",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 形態素分析ライブラリーMeCab と 辞書(mecab-ipadic-NEologd)のインストール \n",
        "!apt-get -q -y install sudo file mecab libmecab-dev mecab-ipadic-utf8 git curl python-mecab > /dev/null\n",
        "!git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git > /dev/null \n",
        "!echo yes | mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd -n > /dev/null 2>&1\n",
        "!pip install mecab-python3 > /dev/null\n",
        "\n",
        "# シンボリックリンクによるエラー回避\n",
        "!ln -s /etc/mecabrc /usr/local/etc/mecabrc"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'mecab-ipadic-neologd'...\n",
            "remote: Enumerating objects: 75, done.\u001b[K\n",
            "remote: Counting objects: 100% (75/75), done.\u001b[K\n",
            "remote: Compressing objects: 100% (74/74), done.\u001b[K\n",
            "remote: Total 75 (delta 5), reused 54 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (75/75), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6s74Lc1fGnnl"
      },
      "source": [
        "## 分類実験\n",
        "LiveDoor News Corpus のデータを対象として９クラス分類問題を解く"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnzykrOm-xZd",
        "outputId": "89446c03-eb47-4bb9-8626-b4894e091551",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 742
        }
      },
      "source": [
        "#coding:utf-8\n",
        "\n",
        "'''\n",
        "livedoorニュースコーパスの分類 (9クラス問題)\n",
        "https://www.rondhuit.com/download.html#ldcc\n",
        "'''\n",
        "import numpy as np\n",
        "import MeCab\n",
        "import gensim\n",
        "import glob\n",
        "import sys\n",
        "import datetime\n",
        "\n",
        "mecab = MeCab.Tagger(\"-Ochasen\")\n",
        "\n",
        "'''\n",
        "文を形態素に分割する関数\n",
        "'''\n",
        "def tokenizer(text, acceptPOS=['名詞', '形容詞', '動詞']):\n",
        "\tresult = list()\n",
        "\tnode = mecab.parse(text)\n",
        "\tfor r in node.split('\\n'):\n",
        "\t\tsurface = r.split('\\t')[0]\n",
        "\t\tif surface == 'EOS': break\n",
        "\n",
        "\t\tfeatures = r.split('\\t')[3]\n",
        "\t\tif len(surface) > 1:\n",
        "\t\t\tresult.append(surface)\n",
        "\n",
        "\treturn result\n",
        "\n",
        "'''\n",
        "データセットの読み込み\n",
        "'''\n",
        "def read_dataset():\n",
        "\tdocs = [] # 文書集合\n",
        "\tlabels = [] # ラベル集合\n",
        "\n",
        "\t# livedoorニュースコーパスにて提供されているニュース記事のクラス\n",
        "\tclass_name = ['it-life-hack', 'movie-enter', 'sports-watch',\n",
        "\t\t\t\t  'kaden-channel', 'peachy', 'topic-news',\n",
        "\t\t\t\t  'dokujo-tsushin', 'livedoor-homme', 'smax']\n",
        "\n",
        "\tprint(\"=== read dataset ===\", file=sys.stderr)\n",
        "\n",
        "\tfor cn in class_name:\n",
        "\t\tprint(\"*** reading {} ***\".format(cn), file=sys.stderr)\n",
        "\t\n",
        "\t\tpaths = glob.glob(r'/content/drive/My Drive/Semi2020/LivedoorNewsCorpus/text/{}/*.txt'.format(cn))\n",
        "\t\tfor path in paths:\n",
        "\t\t\twith open(path, 'r') as f:\n",
        "\t\t\t\ttokens = []\n",
        "\t\t\t\t# 各ファイルの冒頭３行はヘッダゆえ，それらを読み飛ばす\n",
        "\t\t\t\tfor line in f.readlines()[3:]:\n",
        "\t\t\t\t\tsentence = line.rstrip('\\n')\n",
        "\t\t\t\t\ttokens.extend([token for token in tokenizer(sentence)])\n",
        "\t\t\t\tdocs.append(tokens)\n",
        "\t\t\t\tlabels.append(class_name.index(cn))\n",
        "\n",
        "\tprint(\"Number of documents:\", len(docs), file=sys.stderr)\n",
        "\n",
        "\treturn docs, np.array(labels)\n",
        "\n",
        "'''\n",
        "特徴抽出\n",
        " gensim を用いた形態素のマッピングと前処理\n",
        " gensim を用いた低頻度語と高頻度語の除去\n",
        " gensim を用いた特徴抽出\n",
        "'''\n",
        "def feature_extraction(docs):\n",
        "\t# 単語と単語 ID のマッピング\n",
        "\tprint(\"=== mapping ID with word ===\", file=sys.stderr)\n",
        "\tdictionary = gensim.corpora.Dictionary(docs)\n",
        "\tprint(\"original voc. size:\",len(dictionary.keys()), file=sys.stderr)\n",
        "\n",
        "\tprint(\"--- example of word mapping ---\", file=sys.stderr)\n",
        "\tcounter = 0\n",
        "\tfor k in dictionary.token2id.keys():\n",
        "\t\tprint(k, dictionary.token2id[k], file=sys.stderr)\n",
        "\t\tcounter += 1\n",
        "\t\tif counter == 20: break\n",
        "\t\n",
        "\t# 低頻度語と高頻度語の除去\n",
        "\t# 出現回数が 10回未満，または単語が 30% の文書に登場したとき，その他を除外\n",
        "\tdictionary.filter_extremes(no_below=10, no_above=0.3)\n",
        "\n",
        "\tprint(\"reduced voc. size:\", len(dictionary.keys()), file=sys.stderr)\n",
        "\n",
        "\t# TF-IDF 特徴量の抽出\n",
        "\tprint(\"=== feature extraction ===\", file=sys.stderr)\n",
        "\tcorpus = [dictionary.doc2bow(doc) for doc in docs] # TFを記録したコーパス\n",
        "\t# print corpus\n",
        "\n",
        "\ttfidfModel = gensim.models.TfidfModel(corpus)\n",
        "\tcorpus_tfidf = tfidfModel[corpus] #TF-IDFを記録したコーパス\n",
        "\n",
        "\t# 文書-単語行列：features には TF-IDF値が格納されている\n",
        "\tfeatures = gensim.matutils.corpus2dense(corpus_tfidf, num_terms=len(dictionary)).T\n",
        "\n",
        "\treturn features\n",
        "\n",
        "'''\n",
        "分類\n",
        "'''\n",
        "def classification(features, labels):\n",
        "\tprint(\"=== training and testing the classifier ===\", file=sys.stderr)\n",
        "\n",
        "\tfrom sklearn.model_selection import train_test_split\n",
        "\tfrom sklearn.neural_network import MLPClassifier\n",
        "\n",
        "\t# データセットを学習データとテストデータに分割\n",
        "\tx_train, x_test, y_train, y_test = train_test_split(features, labels, train_size=0.8, test_size=0.2)\n",
        "\n",
        "\t# 識別器(multi-layer perceptron)の定義\n",
        "\t# classifier = MLPClassifier(hidden_layer_sizes=(200, 100, 50, 10), verbose=True)\n",
        "\tclassifier = MLPClassifier(hidden_layer_sizes=(200, 10), verbose=True)\n",
        "\t\n",
        "\t# from sklearn.ensemble import RandomForestClassifier\n",
        "\t# classifier = RandomForestClassifier(n_estimators=100)\n",
        "\n",
        "\t# 学習\n",
        "\tclassifier.fit(x_train, y_train)\n",
        "\n",
        "\t# 予測 (分類)\n",
        "\tprint(\"accuracy:\", classifier.score(x_test, y_test))\n",
        "\n",
        "# データセットの読み込み：docs は文書集合，labels はラベル集合\n",
        "docs, labels = read_dataset()\n",
        "\n",
        "'''\n",
        "docs の中身のイメージ\n",
        "[['今朝', '雪', '積もっ', 'い'],\n",
        "\t['雪','いえ','アナ', '雪', '女王'],\n",
        "\t['雪', 'いえ', '白雪姫'],\n",
        "\t['明日', '雪', '降る']\n",
        "]\n",
        "'''\n",
        "\n",
        "# 特徴抽出\n",
        "features = feature_extraction(docs)\n",
        "print(\"shape of features\", features.shape, file=sys.stderr)\n",
        "print(\"shape of labels\", labels.shape, file=sys.stderr)\n",
        "\n",
        "# 分類実験\n",
        "classification(features, labels)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of documents: 0\n",
            "=== mapping ID with word ===\n",
            "original voc. size: 0\n",
            "--- example of word mapping ---\n",
            "reduced voc. size: 0\n",
            "=== feature extraction ===\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:502: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  result = np.column_stack(sparse2full(doc, num_terms) for doc in corpus)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-acb7449f9bbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;31m# 特徴抽出\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_extraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"shape of features\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"shape of labels\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-acb7449f9bbc>\u001b[0m in \u001b[0;36mfeature_extraction\u001b[0;34m(docs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;31m# 文書-単語行列：features には TF-IDF値が格納されている\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus2dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_terms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/matutils.py\u001b[0m in \u001b[0;36mcorpus2dense\u001b[0;34m(corpus, num_terms, num_docs, dtype)\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mdocno\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnum_docs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse2full\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_terms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mcolumn_stack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/shape_base.py\u001b[0m in \u001b[0;36mcolumn_stack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    654\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[0marrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 656\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twLe27SnAJA0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}